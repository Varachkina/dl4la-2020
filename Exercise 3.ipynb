{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/ner/gmb.csv ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/gmb.csv > data/ner/gmb.csv; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/ner/gmb.csv\",encoding = 'latin1')\n",
    "data = data.fillna(method = 'ffill')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_sentence(object):\n",
    "    def __init__(self,data):\n",
    "        self.n_sent=1\n",
    "        self.data = data\n",
    "        agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                     s[\"POS\"].values.tolist(),\n",
    "                                                     s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('demonstrators', 'NNS', 'O'),\n",
       " ('have', 'VBP', 'O'),\n",
       " ('marched', 'VBN', 'O'),\n",
       " ('through', 'IN', 'O'),\n",
       " ('London', 'NNP', 'B-geo'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('protest', 'VB', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('war', 'NN', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('Iraq', 'NNP', 'B-geo'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('demand', 'VB', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('withdrawal', 'NN', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('British', 'JJ', 'B-gpe'),\n",
       " ('troops', 'NNS', 'O'),\n",
       " ('from', 'IN', 'O'),\n",
       " ('that', 'DT', 'O'),\n",
       " ('country', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getter = Get_sentence(data)\n",
    "sentence = getter.sentences\n",
    "sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "\n",
    "words_tag = list(set(data[\"Tag\"].values))\n",
    "\n",
    "word_idx = {w : i+1 for i ,w in enumerate(words)}\n",
    "tag_idx =  {t : i for i ,t in enumerate(words_tag)}\n",
    "\n",
    "num_words = len(words)\n",
    "num_words_tag = len(words_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "max_len = 50\n",
    "X = [[word_idx[w[0]] for w in s] for s in sentence]\n",
    "X = pad_sequences(maxlen = max_len,sequences = X,padding = 'post',value = num_words-1)\n",
    "y = [[tag_idx[w[2]] for w in s] for s in sentence]\n",
    "y = pad_sequences(maxlen = max_len,sequences = y,padding = 'post',value = tag_idx['O'])\n",
    "y = [to_categorical(i,num_classes = num_words_tag) for i in  y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26923, 19139, 24867, 23709, 29041,  8430, 27858, 30155,  8430,\n",
       "        29559, 28627, 33672, 12988, 31283, 28807, 17028, 28709, 16229,\n",
       "        15584, 30917, 25100, 13758, 22296,  2732,  7293, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [11953,  6169, 23248, 30220, 29078,  7065, 27463, 31283, 33979,\n",
       "         4527, 15710, 28709, 25904,  4095,    81, 10602, 11036,  5071,\n",
       "         8430,  2453,  5702,  8430, 32447, 15584,  4987,   157, 26057,\n",
       "        27463,  6749, 34361, 32053,  7293, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [13702, 13477, 16229, 18119, 20228, 22352, 14426,  2273, 14172,\n",
       "         8509,  7293, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [10364, 27043, 18144, 18956, 13873, 16235, 15390, 28709,  9266,\n",
       "        28709, 28548, 11472, 11031,   157, 32464, 11036, 27331,  1073,\n",
       "        16779, 11472, 14490,  8430, 33035,  4739, 12988, 28112, 14197,\n",
       "        21827, 29047, 23997, 26906, 23484, 23061, 16567,  7293, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [ 1669, 13873, 16235, 31283, 17086, 29684, 21499,  9135, 29856,\n",
       "        29323, 11036, 27463, 23940, 12988, 16779,  9069, 21085,  2273,\n",
       "        31283, 23940,  5902, 27463,  1504, 28709,  4595, 10719, 28709,\n",
       "        31283, 25870, 11472, 23792,  7293, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [12143, 16235, 34688, 19081,  5480, 15448, 11472, 31283,   570,\n",
       "        24004, 31754, 29047,  4389,   961,  3765, 28059, 14098, 27463,\n",
       "        30695, 11472, 31485,  7293, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [22899,   756, 29323, 11036, 14538, 21428,  7293, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [ 4607, 14013,  9072,  6901,  8430, 31283, 15710, 28709,  5837,\n",
       "        31283, 10364,  3752, 26893, 29047, 22229, 11472, 18119, 14037,\n",
       "        12988, 16649, 20056, 18119, 29419,  7293, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178],\n",
       "       [28313, 30575, 11036,  5875, 17202,  5480, 34372,  8118, 27463,\n",
       "        18119, 13511,   491, 12988, 25265,  7382, 15584,  2152,  8430,\n",
       "        10775, 13212, 11540, 27463, 31283, 24191,  7046, 21842,  7293,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "        35178, 35178, 35178, 35178, 35178]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import LSTM,Embedding,Dense,SimpleRNN\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional\n",
    "from tensorflow.python.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 20)            703580    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 50, 20)            0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 50, 100)           12100     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 50, 17)            1717      \n",
      "=================================================================\n",
      "Total params: 717,397\n",
      "Trainable params: 717,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Input(shape = (max_len,)))\n",
    "model.add(Embedding(input_dim = num_words, output_dim = 20, input_length = max_len))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(SimpleRNN(units=100, return_sequences = True, recurrent_dropout=0.1))\n",
    "#model.add(Bidirectional(LSTM(units=100,return_sequences = True, recurrent_dropout = 0.1)))\n",
    "model.add(TimeDistributed(Dense(num_words_tag,activation = 'softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "675/675 [==============================] - 11s 16ms/step - loss: 0.2854 - accuracy: 0.9438\n",
      "Epoch 2/3\n",
      "675/675 [==============================] - 11s 16ms/step - loss: 0.0740 - accuracy: 0.9796\n",
      "Epoch 3/3\n",
      "675/675 [==============================] - 11s 17ms/step - loss: 0.0538 - accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping \n",
    "\n",
    "history = model.fit(\n",
    "    x_train,np.array(y_train),\n",
    "    batch_size = 64,\n",
    "    epochs = 3,\n",
    "    verbose =1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05665678158402443, 0.9830400347709656]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Evaluation scores per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "Y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "#classification_report(Y_test, y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/challenge.wb.csv > data/ner/challenge.wb.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/challenge.bc.csv > data/ner/challenge.bc.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/challenge.nw.csv > data/ner/challenge.nw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = pd.read_csv(\"data/ner/challenge.wb.csv\", header = 0, names=[\"Sentence #\",\"Word\",\"POS\",\"Tag\"])\n",
    "challenge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = Get_sentence(challenge)\n",
    "sentence = getter.sentences\n",
    "sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "x_challenge = [[word_idx.get(w[0],0) for w in s] for s in sentence]\n",
    "x_challenge = pad_sequences(maxlen = max_len,sequences = x_challenge,padding = 'post',value = num_words-1)\n",
    "y_challenge = [[tag_idx[w[2]] for w in s] for s in sentence]\n",
    "y_challenge = pad_sequences(maxlen = max_len,sequences = y_challenge,padding =\n",
    "                        'post',value = tag_idx['O'])\n",
    "y_challenge = [to_categorical(i,num_classes = num_words_tag) for i in  y_challenge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_challenge[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_challenge, np.array(y_challenge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
