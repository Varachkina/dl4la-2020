{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Virtual Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Aug 31 â€” Sep 4, 2020<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 2\n",
    "\n",
    "In this exercise, we will continue using the amazon reviews dataset, and continue to classify them according to their polarity (positive/negative).\n",
    "\n",
    "But in contrast to exercise 1, we will be using embeddings and a convoluational neural network.\n",
    "\n",
    "----\n",
    "\n",
    "The first two steps ensure that you have the data set at the correct location (but you should already have them from Exercise 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! if ! [[ -f data/amazon/train.ft.txt.bz2 ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/amazon/train.ft.txt.bz2 > data/amazon/train.ft.txt.bz2; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/amazon/test.ft.txt.bz2 ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/amazon/test.ft.txt.bz2 > data/amazon/test.ft.txt.bz2; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines a function to read in the first 100000 reviews of both training and testing data set. This is the same as in Exercise 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "\n",
    "def get_labels_and_texts(file, limit=100000):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    lineNumber = 0\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "        lineNumber = lineNumber + 1\n",
    "        if lineNumber >= limit and limit > 0:\n",
    "          break\n",
    "    return np.array(labels), texts\n",
    "\n",
    "train_labels, train_texts = get_labels_and_texts('data/amazon/train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('data/amazon/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tokenization\n",
    "\n",
    "Since we are not using a bag-of-words representation and a word-document matrix, we need to keep the sequence of the words intact. Therefore, we first need to **tokenize** all the texts. The class [`Tokenizer` from Keras](https://keras.io/api/preprocessing/text/#tokenizer-class) does this, and also maintains an index of strings to numbers. I.e., by using this class (which needs to be fitted to the training data), we get a sequence of integers as a result.\n",
    "\n",
    "Three steps are needed for use:\n",
    "1. Initialization of a `Tokenizer` object with reasonable parameters (e.g., you can restrict the number of words to distinguish with the parameter `num_words`).\n",
    "2. Fitted the tokenizer to the training data (to learn an actual mapping from surfaces to integers).\n",
    "3. Convert the train and test sequences to integer sequences. The results should be in variables called `train_texts` and `test_texts`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_FEATURES = 12000\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "test_texts = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Padding\n",
    "\n",
    "Because the input data needs to have the same shape for all sentences, we need to \"fill\" shorter sentences. This process is called **padding**, and there is also a ready-made function for this: [`pad_sequences()` from Keras](https://keras.io/api/preprocessing/timeseries/#padsequences-function). The function needs only one argument: The length to which sequences should be padded. This of course can be a made-up number, but this is risky, because sentences might be longer than you think. It's better to find out the length of the longest sentence in the training data first, and pad the data to this length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
    "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
    "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Definition\n",
    "\n",
    "Now we can define the model. For starters, you should define a model with the following layers: 1. Embedding, 2. Convolutional, 3. Flatten, 4. Dense. For compilation, you can pick a binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape=(MAX_LENGTH,)))\n",
    "model.add(layers.Embedding(input_dim=MAX_FEATURES, output_dim=64))\n",
    "model.add(layers.Conv1D(filters=10, kernel_size=5, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Training and Evaluation\n",
    "\n",
    "This might take a long time, because the entire data set is quite large. If you are impatient, it might be a good idea to only use a subset of the data for training (e.g., the first 5000 reviews or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INSTANCES=5000\n",
    "\n",
    "history = model.fit(train_texts, train_labels, \n",
    "    batch_size=128, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_texts, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
